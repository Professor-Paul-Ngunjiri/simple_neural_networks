# Neural Network Training

This project implements a simple **feedforward neural network** using **Python** and **NumPy**. The network is trained using **backpropagation** and visualizes the **error reduction over time** using **Matplotlib**.

---

## ğŸ“Œ Project Overview

This neural network consists of:
âœ… **An input layer** with two input values
âœ… **A hidden layer** with two neurons
âœ… **An output layer** with one neuron
âœ… **Sigmoid activation function** for non-linearity
âœ… **Backpropagation** for training and adjusting weights
âœ… **Error visualization** using Matplotlib

The training process updates weights over multiple epochs to minimize error and improve prediction accuracy.

---

## ğŸ› ï¸ Technologies Used

This project is built using:
- **Python** (Programming Language)
- **NumPy** (For numerical computations)
- **Matplotlib** (For visualizing training progress)

---

## ğŸ”§ Installation Guide

To run this project on your local machine, follow these steps:

### 1ï¸âƒ£ Clone the Repository
```sh
git clone https://github.com/Professor-Paul-Ngunjiri/simple_neural_networks.git
cd neural-network-training
```

### 2ï¸âƒ£ Install Dependencies
```sh
pip install numpy matplotlib
```

### 3ï¸âƒ£ Run the Script
```sh
python neural_network_backprop_simulation.py
```

---

## ğŸ“ Code Explanation

The script implements a basic **feedforward neural network** with **one hidden layer** and trains it using **backpropagation**.

### ğŸ”¹ Core Functions

#### Sigmoid Activation Function
```python
# Sigmoid function to introduce non-linearity
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

#### Derivative of Sigmoid (Used for Backpropagation)
```python
# Derivative of sigmoid function
def sigmoid_derivative(x):
    return x * (1 - x)
```

### ğŸ”¹ Training Process
The network undergoes multiple **epochs** (iterations) where:
1. It **calculates weighted inputs** to the hidden and output layers.
2. It **applies the sigmoid activation function** to introduce non-linearity.
3. It **computes the error** (difference between predicted and actual output).
4. It **adjusts weights** using **backpropagation** to minimize the error.

```python
# Compute Error
target = 1
error = target - final_output

# Adjust weights using backpropagation
w1 += learning_rate * delta_output * hidden_output_1
w2 += learning_rate * delta_output * hidden_output_2
```

---

## ğŸ“Š Expected Output

During training, the network prints output showing:
- Neuron inputs & outputs
- Error reduction per epoch

```
Epoch 1/100
Hidden Neuron 1 Input: 0.32, Output: 0.58
Hidden Neuron 2 Input: 0.45, Output: 0.61
Final Neuron Input: 0.52, Output: 0.63
Error: 0.37
```

---

## ğŸ“ˆ Training Progress Visualization

The **error reduction over time** is plotted using Matplotlib:

```python
plt.plot(range(epochs), errors, label='Error Reduction', color='red')
plt.xlabel('Epochs')
plt.ylabel('Error')
plt.title('Neural Network Learning Progress')
plt.legend()
plt.grid()
plt.show()
```

### Example Graph Output:
![Error Reduction Graph](error_graph.png)

This graph shows how the **error decreases** as the neural network learns.

---

## ğŸ“š Further Improvements

Potential enhancements for this project include:
âœ… **Increasing hidden layers** to make the model more complex
âœ… **Using different activation functions** like ReLU or Tanh
âœ… **Implementing a larger dataset** for better generalization
âœ… **Introducing learning rate decay** for optimized training

---

## ğŸ”— Useful Resources
- [NumPy Documentation](https://numpy.org/doc/)
- [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)
- [Backpropagation Algorithm Explained](https://en.wikipedia.org/wiki/Backpropagation)

---

## ğŸ’¡ About the Author

ğŸ‘¤ **Professor-Paul-Ngunjiri**  
ğŸ”— [GitHub](https://github.com/Professor-Paul-Ngunjiri)
ğŸ“§ ngunjiripaul485@gmail.com  

---

## ğŸ“œ License
This project is licensed under the **GNU GENERAL PUBLIC LICENSE**. Feel free to use and modify the code as needed!

```md
GNU GENERAL PUBLIC LICENSE
Copyright (c) 2025
```

